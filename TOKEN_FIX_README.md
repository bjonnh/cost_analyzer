# Token Data Fix

## The Issue

The token data wasn't showing up in the dashboard because the code was looking for the `usage` field at the wrong level in the JSONL structure.

### Expected Structure (incorrect)
```json
{
  "costUSD": 0.05,
  "usage": {
    "input_tokens": 100,
    ...
  }
}
```

### Actual Structure (correct)
```json
{
  "costUSD": 0.05,
  "message": {
    "usage": {
      "input_tokens": 100,
      "output_tokens": 50,
      "cache_creation_input_tokens": 10,
      "cache_read_input_tokens": 5,
      "service_tier": "standard"
    }
  }
}
```

## The Fix

The data processor has been updated to look for token data in the correct location:
`entry['message']['usage']` instead of `entry['usage']`

## How to Apply the Fix

1. **Reprocess your JSONL files** to extract the token data:
   ```bash
   python reprocess_tokens.py
   # or
   ./reprocess_tokens.py
   ```
   
   This will reload all JSONL files and properly extract the token data.

2. **Verify the fix worked**:
   ```bash
   python verify_tokens.py
   ```
   
   This will show you if token data is now properly loaded.

3. **Run the dashboard**:
   ```bash
   ./analyzer.py
   ```
   
   The Tokens tab should now show your token usage data!

## Diagnostic Tools

If you still have issues, use these diagnostic tools:

- `diagnose_tokens.py` - Comprehensive diagnostic of token data pipeline
- `show_token_data.py` - Display raw token data from database
- `examine_jsonl_structure.py` - Examine JSONL file structure
- `test_token_extraction.py` - Test token extraction logic

## Notes

- Token data is only available in JSONL entries that have a cost (costUSD > 0)
- The token data includes:
  - `input_tokens` - Tokens sent to Claude
  - `output_tokens` - Tokens generated by Claude
  - `cache_creation_input_tokens` - Tokens used to create cache
  - `cache_read_input_tokens` - Tokens read from cache
  - `service_tier` - The service tier used (e.g., "standard")